# -*- coding: utf-8 -*-
"""pyannote.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eKPQgpbnhzWSQmWc1g2P5eVz_8HAmDdC
"""

pip install pyannote.audio

from pyannote.audio import Pipeline
pipeline = Pipeline.from_pretrained(
    "pyannote/speaker-diarization-3.1",
    use_auth_token="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")

import torch
import torchaudio
from pydub.utils import mediainfo
import subprocess
import tempfile
import os

# Step 1: Define the helper
def ensure_wav_format(input_path: str, sample_rate: int = 16000, channels: int = 1) -> str:
    """Ensures the input audio file is in WAV format and converts if necessary.
       Supports various input formats including mp3, amr, etc."""
    try:
        info = mediainfo(input_path)
        if info.get('format_name') == 'wav' and info.get('codec_name') == 'pcm_s16le':
            return input_path
    except Exception:
        pass  # If mediainfo fails, fallback to ffmpeg

    fd, output_path = tempfile.mkstemp(suffix=".wav")
    os.close(fd)

    command = [
        "ffmpeg", "-y", "-i", input_path,
        "-ar", str(sample_rate),
        "-ac", str(channels),
        "-c:a", "pcm_s16le",
        output_path
    ]
    result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    if result.returncode != 0:
        raise RuntimeError(f"FFmpeg failed:\n{result.stderr.decode()}")
    return output_path

# Step 2: Use the helper in your pipeline
pipeline.to(torch.device("cuda"))

original_audio_path = "2.mp3"  # or mp3, amr, etc.
cleaned_audio_path = ensure_wav_format(original_audio_path)

audio, sample_rate = torchaudio.load(cleaned_audio_path)

diarization = pipeline({"waveform": audio, "sample_rate": sample_rate}, num_speakers=2)

segments = []
for turn, _, speaker in diarization.itertracks(yield_label=True):
    segement_part = {
        "start": turn.start,
        "end": turn.end,
        "speaker": speaker
    }
    segments.append(segement_part)

# Optional: delete temp file if it was converted
if cleaned_audio_path != original_audio_path:
    os.remove(cleaned_audio_path)

from pydub import AudioSegment
import torch
import torchaudio
import io
import os

# Step 1: Ensure it's a proper WAV file (use the helper from previous cell)
audio_path = "2.mp3"
cleaned_audio_path = ensure_wav_format(audio_path)

# Step 2: Load using AudioSegment
full_audio = AudioSegment.from_wav(cleaned_audio_path)

# Step 3: Process diarized segments
chunk_tensors = []

for i, seg in enumerate(segments):
    start_ms = int(seg["start"] * 1000)
    end_ms = int(seg["end"] * 1000)
    speaker = seg["speaker"]

    # Slice the chunk
    audio_chunk = full_audio[start_ms:end_ms]

    # Export to in-memory buffer
    buffer = io.BytesIO()
    audio_chunk.export(buffer, format="wav")
    buffer.seek(0)

    # Load waveform from buffer
    waveform, sr = torchaudio.load(buffer)

    if sr != 16000:
        waveform = torchaudio.functional.resample(waveform, sr, 16000)

    if waveform.shape[0] > 1:
        waveform = torch.mean(waveform, dim=0, keepdim=True)

    chunk_tensors.append({
        "waveform": waveform,
        "sampling_rate": 16000,
        "speaker": speaker,
        "index": i
    })

# Step 4 (Optional): Clean up temp if converted
if cleaned_audio_path != audio_path:
    os.remove(cleaned_audio_path)

from transformers import SeamlessM4Tv2ForSpeechToText, SeamlessM4TTokenizer, SeamlessM4TFeatureExtractor

model = SeamlessM4Tv2ForSpeechToText.from_pretrained("ai4bharat/indic-seamless").to("cuda")
processor = SeamlessM4TFeatureExtractor.from_pretrained("ai4bharat/indic-seamless")
tokenizer = SeamlessM4TTokenizer.from_pretrained("ai4bharat/indic-seamless")

transcriptions = []

for chunk in chunk_tensors:
    waveform = chunk["waveform"]
    speaker = chunk["speaker"]
    index = chunk["index"]

    waveform = waveform.squeeze(0).cpu()

    # Skip too-short waveforms
    if waveform.numel() < 3200:  # Less than 0.2 seconds at 16kHz
        print(f"Skipping chunk {index} from {speaker}: too short ({waveform.numel()} samples)")
        continue

    # Prepare model inputs
    inputs = processor(waveform, sampling_rate=16000, return_tensors="pt").to("cuda")

    with torch.no_grad():
        generated_tokens = model.generate(**inputs, tgt_lang="hin")[0].cpu().numpy().squeeze()

    text = tokenizer.decode(generated_tokens, skip_special_tokens=True, clean_up_tokenization_spaces=True)

    transcriptions.append({
        "speaker": speaker,
        "index": index,
        "text": text
    })

transcriptions = sorted(transcriptions, key=lambda x: x["index"])

for t in transcriptions:
    print(f"[{t['speaker']}]: {t['text']}")